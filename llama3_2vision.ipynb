{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NandiniLReddy/Llama-3.2/blob/main/llama3_2vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama 3.2 deployed by Nandini Lokesh Reddy"
      ],
      "metadata": {
        "id": "x19AYLe3UGEd"
      },
      "id": "x19AYLe3UGEd"
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"HUGGING FACE TOKEN\")"
      ],
      "metadata": {
        "id": "RUdKyYM2U2iC"
      },
      "id": "RUdKyYM2U2iC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "y121JQz2U6gs"
      },
      "id": "y121JQz2U6gs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usecase: Text generation"
      ],
      "metadata": {
        "id": "dCOp8otVUMjz"
      },
      "id": "dCOp8otVUMjz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e2faaee-6042-4ea3-b7ff-0c15e097fc11",
      "metadata": {
        "tags": [],
        "id": "4e2faaee-6042-4ea3-b7ff-0c15e097fc11",
        "outputId": "eb3379f5-bb94-4b93-f01b-29cfaff08eb0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'system', 'content': 'You are a pirate chatbot who always responds in pirate speak!'}, {'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \"Arrrr, ye be askin' about meself, eh? Alright then, matey! I be Captain Blackbeak Betty, the most feared and infamous pirate to ever sail the Seven Seas. Me and me crew o' scurvy dogs have been plunderin' and pillagin' fer years, bringin' riches and glory to our names. Me trusty parrot, Polly, be me right-hand scallywag, and me loyal cutlass, Cutlass Cutie, be me best mate. We be sailin' the Caribbean, searchin' fer hidden treasure and fightin' off any landlubbers who get in our way. So hoist the sails and set course fer adventure with me and me crew, or walk the plank, matey!\"}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "# Specify the device to use GPU (device 0 by default)\n",
        "device = 0 if torch.cuda.is_available() else -1  # -1 indicates CPU\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device=device  # Use GPU if available\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"}\n",
        "]\n",
        "\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256\n",
        ")\n",
        "\n",
        "print(outputs[0][\"generated_text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usecase: Topic Generation"
      ],
      "metadata": {
        "id": "ZMMzk1zfUTH5"
      },
      "id": "ZMMzk1zfUTH5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "749c6ba8-08ff-42b2-b555-0edec8460b35",
      "metadata": {
        "tags": [],
        "id": "749c6ba8-08ff-42b2-b555-0edec8460b35",
        "outputId": "62fb48af-dee6-44b4-9596-deca0ba2315a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'system', 'content': 'Generate a list of topics related to the following context:'}, {'role': 'user', 'content': 'L&D practitioners spend a lot of time and effort creating excellent leadership development programs, yet year after year companies report leadership as lacking.\\nThe main reason for this? Once the participants leave the training program, it’s difficult to hold them accountable and ensure the learnings are applied at work.\\nLeveraging technologies like AI, NLU and NLP can help address this by serving as ongoing mentors to guide leaders to foster organisations with engaged employees..'}, {'role': 'assistant', 'content': \"Here's a list of topics related to the given context:\\n\\n**Leadership Development Program Challenges**\\n\\n1. **Lack of Leadership Skills**: Identifying and developing effective leadership skills in employees.\\n2. **Retention and Engagement**: Keeping employees engaged and motivated\"}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "# Use GPU if available, otherwise CPU\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "# Initialize the text-generation pipeline with the specified model\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Example of a user-provided context\n",
        "user_input = \"\"\"L&D practitioners spend a lot of time and effort creating excellent leadership development programs, yet year after year companies report leadership as lacking.\n",
        "The main reason for this? Once the participants leave the training program, it’s difficult to hold them accountable and ensure the learnings are applied at work.\n",
        "Leveraging technologies like AI, NLU and NLP can help address this by serving as ongoing mentors to guide leaders to foster organisations with engaged employees..\"\"\"\n",
        "\n",
        "# System instructs the model to generate topics based on the context\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Generate a list of topics related to the following context:\"},\n",
        "    {\"role\": \"user\", \"content\": user_input}\n",
        "]\n",
        "\n",
        "# Generate output with the specified number of new tokens\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=50  # Adjust token limit based on typical length needed for topic lists\n",
        ")\n",
        "\n",
        "# Print the generated topics\n",
        "print(outputs[0][\"generated_text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11B Vision model: image description"
      ],
      "metadata": {
        "id": "L7i3Tig5Ueao"
      },
      "id": "L7i3Tig5Ueao"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba7da214-3923-4f78-8ae6-5042efaa31c9",
      "metadata": {
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "c1ce6cab8a874c7f9a00bcabfc5bbf7d",
            "1624d11f4b624884b6842b547dc9e818",
            "89278c74617f48f5b1af6403e24eff91",
            "0235dd7f2b4144d2a7b682f53f295933",
            "827d27458c374b3e83dfb05d7615bf0d",
            "6b4b579f74f144ab98f7b83cece5280f",
            "6fd00a19cb384a5b8cca928aa8527d6c"
          ]
        },
        "id": "ba7da214-3923-4f78-8ae6-5042efaa31c9",
        "outputId": "314085b6-ddc8-44db-8aef-c9f0cbe005f6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1ce6cab8a874c7f9a00bcabfc5bbf7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1624d11f4b624884b6842b547dc9e818",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89278c74617f48f5b1af6403e24eff91",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0235dd7f2b4144d2a7b682f53f295933",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/55.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "827d27458c374b3e83dfb05d7615bf0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b4b579f74f144ab98f7b83cece5280f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fd00a19cb384a5b8cca928aa8527d6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.json:   0%|          | 0.00/557 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "<|image|>If I had to write a haiku for this one, it would be: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Here is a haiku for the image:\n",
            "\n",
            "A rabbit in a coat\n",
            "Stands on a dirt road, smiling\n",
            "Springtime in the air<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the model\n",
        "model = MllamaForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Load the processor\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Load an image from the web\n",
        "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "# Create a message list that includes an image and a text prompt\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": [\n",
        "        {\"type\": \"image\"},\n",
        "        {\"type\": \"text\", \"text\": \"If I had to write a haiku for this one, it would be: \"}\n",
        "    ]}\n",
        "]\n",
        "\n",
        "# Prepare inputs using the processor\n",
        "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = processor(image, input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate output from the model\n",
        "output = model.generate(**inputs, max_new_tokens=30)\n",
        "print(processor.decode(output[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd4b4238-3b2c-4cb1-85dc-0a045b295d9e",
      "metadata": {
        "id": "dd4b4238-3b2c-4cb1-85dc-0a045b295d9e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mistral12b",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}